{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, prediction_win):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.prediction_win = prediction_win\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx]\n",
    "\n",
    "        input_windows = windows[:-self.prediction_win]  \n",
    "        target_windows = windows[-self.prediction_win:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        # print(f'training_key_counter{len(target_key_counter)}')\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "\n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        # print (f'torch.tensor(seq_indices, dtype=torch.long).shape{torch.tensor(seq_indices, dtype=torch.long).shape}')\n",
    "        # print (f'torch.tensor(target_padded, dtype=torch.long.shape{torch.tensor(target_padded, dtype=torch.long).shape}')\n",
    "\n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                # print(f'target_set: {target_set}\\n')\n",
    "                # print(f'pred_set: {pred_set}')    \n",
    "                \n",
    "        \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "                # print(f'len(intersection{tp}, target_set_len{len(target_set)}, pred_set{len(pred_set)})')\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        # window_step = max(1, self.window_size // 10)  # 10% \n",
    "        window_step = max(1, self.window_size // 4)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length - self.prediction_win + 1):\n",
    "            seq = windows[i:i + self.sequence_length + self.prediction_win]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "   \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 计算sigmoid概率\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 基础焦点损失\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        # 'data_file': 'temporal_ycsb_workload_part1.csv',\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 5000,             \n",
    "        'sequence_length': 100,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        # 'top_k_hot_keys': 2000,\n",
    "        'batch_size': 8,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 50\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "#device\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新处理数据，提高数据利用率 && 改进验证模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 500\n",
      "  sequence_length: 300\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 16\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 300\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 3000000 operations in 0.15 seconds\n",
      "Using device: cuda\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 3000000\n",
      "Window size: 500 keys, Step: 500 keys\n",
      "Created 6000 windows with overlap\n",
      "\n",
      "Data Statistics:\n",
      "  Total unique keys: 100000\n",
      "  Total windows: 6000\n",
      "  Avg unique keys per window: 435.55\n",
      "  Max unique keys per window: 461\n",
      "  Min unique keys per window: 405\n",
      "\n",
      "Creating training sequences with 300 historical windows...\n",
      "Created 5701 training sequences in 0.66 seconds\n",
      "\n",
      "Creating data loaders...\n",
      "  Train samples: 3990\n",
      "  Validation samples: 570\n",
      "  Test samples: 1141\n",
      "Data loaders created in 0.00 seconds\n",
      "\n",
      "=== Training Model ===\n",
      "\n",
      "Initializing efficient model with vocabulary size: 100001\n",
      "Model parameters: 5,928,417 total, 5,928,417 trainable\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5] - 156.03s\n",
      "  Train Loss: 0.1404, Val Loss: 0.0943, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.3975  prec: 0.3975  f1: 0.3975  accu: 0.3975  \n",
      "\n",
      "Epoch [2/5] - 154.00s\n",
      "  Train Loss: 0.0829, Val Loss: 0.0921, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.4074  prec: 0.4074  f1: 0.4074  accu: 0.4074  \n",
      "\n",
      "Epoch [3/5] - 149.37s\n",
      "  Train Loss: 0.0805, Val Loss: 0.0926, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.4078  prec: 0.4078  f1: 0.4078  accu: 0.4078  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 627\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;66;03m# model_path = 'hot_key_prsedictor.pth'\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# predictor.save_model(model_path)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \n\u001b[1;32m    624\u001b[0m     \u001b[38;5;66;03m# print(f\"\\nTraining completed! Model saved to {model_path}\")\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 627\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 609\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    604\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mcreate_data_loaders(\n\u001b[1;32m    605\u001b[0m     sequences, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    606\u001b[0m )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 609\u001b[0m train_losses, val_losses, val_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# print(\"\\n=== Evaluating Model ===\")\u001b[39;00m\n\u001b[1;32m    619\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader, k_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 361\u001b[0m, in \u001b[0;36mKeyPredictor.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    358\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    359\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    362\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    364\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, target_windows_size):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.target_windows_size = target_windows_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx] if idx < len(self.sequences) else []\n",
    "        next_windows = self.sequences[idx + 1] if idx + 1 < len(self.sequences) else []\n",
    "\n",
    "        input_windows = windows \n",
    "        target_windows = next_windows[-self.target_windows_size:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        # print(f'training_key_counter{len(target_key_counter)}')\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "\n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        # print (f'torch.tensor(seq_indices, dtype=torch.long).shape{torch.tensor(seq_indices, dtype=torch.long).shape}')\n",
    "        # print (f'torch.tensor(target_padded, dtype=torch.long.shape{torch.tensor(target_padded, dtype=torch.long).shape}')\n",
    "\n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                # print(f'target_set: {target_set}\\n')\n",
    "                # print(f'pred_set: {pred_set}')    \n",
    "                \n",
    "        \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "                # print(f'len(intersection{tp}, target_set_len{len(target_set)}, pred_set{len(pred_set)})')\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        # window_step = max(1, self.window_size // 10)不应该重复呀\n",
    "        window_step = max(1, self.window_size)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length + 1):\n",
    "            seq = windows[i:i + self.sequence_length]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "   \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 计算sigmoid概率\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 基础焦点损失\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        # 'data_file': 'temporal_ycsb_workload_part1.csv',\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 500,             \n",
    "        'sequence_length': 300,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        # 'top_k_hot_keys': 2000,\n",
    "        'batch_size': 16,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 300\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "#device\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "gpu_id = 0                     \n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移除后windows百分之20的冷键,但是效果不太好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 500\n",
      "  sequence_length: 300\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 16\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 300\n",
      "  remove_low_freq_ratio: 0.05\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 3000000 operations in 0.15 seconds\n",
      "Using device: cuda\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 3000000\n",
      "Window size: 500 keys, Step: 500 keys\n",
      "Created 6000 windows with overlap\n",
      "\n",
      "Data Statistics:\n",
      "  Total unique keys: 100000\n",
      "  Total windows: 6000\n",
      "  Avg unique keys per window: 435.55\n",
      "  Max unique keys per window: 461\n",
      "  Min unique keys per window: 405\n",
      "  Remove low frequency ratio: 0.05\n",
      "\n",
      "Creating training sequences with 300 historical windows...\n",
      "Created 5701 training sequences in 0.66 seconds\n",
      "\n",
      "Creating data loaders...\n",
      "  Train samples: 3990\n",
      "  Validation samples: 570\n",
      "  Test samples: 1141\n",
      "Data loaders created in 0.00 seconds\n",
      "\n",
      "=== Training Model ===\n",
      "\n",
      "Initializing efficient model with vocabulary size: 100001\n",
      "Model parameters: 5,928,417 total, 5,928,417 trainable\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5] - 195.97s\n",
      "  Train Loss: 0.1406, Val Loss: 0.0923, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.4040  prec: 0.4040  f1: 0.4040  accu: 0.4040  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 653\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# model_path = 'hot_key_prsedictor.pth'\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# predictor.save_model(model_path)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# print(f\"\\nTraining completed! Model saved to {model_path}\")\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 653\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 635\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    630\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mcreate_data_loaders(\n\u001b[1;32m    631\u001b[0m     sequences, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    632\u001b[0m )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 635\u001b[0m train_losses, val_losses, val_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# print(\"\\n=== Evaluating Model ===\")\u001b[39;00m\n\u001b[1;32m    645\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader, k_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 375\u001b[0m, in \u001b[0;36mKeyPredictor.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    372\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    373\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    376\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    378\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, target_windows_size, remove_low_freq_ratio=0.05):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.target_windows_size = target_windows_size\n",
    "        self.remove_low_freq_ratio = remove_low_freq_ratio  # 要移除的低频键比例\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx] if idx < len(self.sequences) else []\n",
    "        next_windows = self.sequences[idx + 1] if idx + 1 < len(self.sequences) else []\n",
    "\n",
    "        input_windows = windows \n",
    "        target_windows = next_windows[-self.target_windows_size:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            # 移除每个窗口中排名后20%的键\n",
    "            if len(window_keys) > 0:\n",
    "                # 计算每个键的频率\n",
    "                key_counter = Counter(window_keys)\n",
    "                total_keys = len(window_keys)\n",
    "                \n",
    "                # 确定要移除的键数量（后20%）\n",
    "                keys_to_remove = int(total_keys * self.remove_low_freq_ratio)\n",
    "                \n",
    "                # 获取频率最低的键\n",
    "                low_freq_keys = [key for key, count in key_counter.most_common()[:-keys_to_remove-1:-1]] if keys_to_remove > 0 else []\n",
    "                \n",
    "                # 从窗口中移除这些低频键\n",
    "                filtered_keys = [key for key in window_keys if key not in low_freq_keys]\n",
    "            else:\n",
    "                filtered_keys = window_keys\n",
    "            \n",
    "            # 转换为索引并填充\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in filtered_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        # 目标窗口保持不变\n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2, remove_low_freq_ratio=0.2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.remove_low_freq_ratio = remove_low_freq_ratio  # 要移除的低频键比例\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        window_step = max(1, self.window_size)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'remove_low_freq_ratio': self.remove_low_freq_ratio\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        print(f\"  Remove low frequency ratio: {self.remove_low_freq_ratio}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length + 1):\n",
    "            seq = windows[i:i + self.sequence_length]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, \n",
    "                                         self.top_k_hot_keys, self.prediction_win, self.remove_low_freq_ratio)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, \n",
    "                                       self.top_k_hot_keys, self.prediction_win, self.remove_low_freq_ratio)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, \n",
    "                                        self.top_k_hot_keys, self.prediction_win, self.remove_low_freq_ratio)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "   \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 计算sigmoid概率\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 基础焦点损失\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            # 应用相同的低频键移除逻辑\n",
    "            if len(window_keys) > 0:\n",
    "                key_counter = Counter(window_keys)\n",
    "                total_keys = len(window_keys)\n",
    "                keys_to_remove = int(total_keys * self.remove_low_freq_ratio)\n",
    "                low_freq_keys = [key for key, count in key_counter.most_common()[:-keys_to_remove-1:-1]] if keys_to_remove > 0 else []\n",
    "                filtered_keys = [key for key in window_keys if key not in low_freq_keys]\n",
    "            else:\n",
    "                filtered_keys = window_keys\n",
    "            \n",
    "            indices = [self.key_to_idx.get(key, 0) for key in filtered_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats,\n",
    "            'remove_low_freq_ratio': self.remove_low_freq_ratio\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "#         self.remove_low_freq_ratio = checkpoint.get('remove_low_freq_ratio', 0.2)\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 500,             \n",
    "        'sequence_length': 300,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        'batch_size': 16,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 300,\n",
    "        'remove_low_freq_ratio': 0.05  # 移除每个窗口中排名后20%的键\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win'],\n",
    "        remove_low_freq_ratio=config['remove_low_freq_ratio']\n",
    "    )\n",
    "\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟真实的数据去做，这个如果可以的话就可以放数据库里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 500\n",
      "  sequence_length: 300\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 16\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 300\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 3000000 operations in 0.17 seconds\n",
      "Using device: cuda\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 3000000\n",
      "Window size: 500 keys, Step: 500 keys\n",
      "Created 6000 windows with overlap\n",
      "\n",
      "Data Statistics:\n",
      "  Total unique keys: 1448548\n",
      "  Total windows: 6000\n",
      "  Avg unique keys per window: 436.55\n",
      "  Max unique keys per window: 464\n",
      "  Min unique keys per window: 404\n",
      "\n",
      "Creating training sequences with 300 historical windows...\n",
      "Created 5701 training sequences in 1.90 seconds\n",
      "\n",
      "Creating data loaders...\n",
      "  Train samples: 3990\n",
      "  Validation samples: 570\n",
      "  Test samples: 1141\n",
      "Data loaders created in 0.00 seconds\n",
      "\n",
      "=== Training Model ===\n",
      "\n",
      "Initializing efficient model with vocabulary size: 1448549\n",
      "Model parameters: 72,007,269 total, 72,007,269 trainable\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5] - 177.98s\n",
      "  Train Loss: 0.0988, Val Loss: 0.0324, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.3846  prec: 0.3846  f1: 0.3846  accu: 0.3846  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/ming/anaconda3/envs/dl/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 627\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;66;03m# model_path = 'hot_key_prsedictor.pth'\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# predictor.save_model(model_path)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \n\u001b[1;32m    624\u001b[0m     \u001b[38;5;66;03m# print(f\"\\nTraining completed! Model saved to {model_path}\")\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 627\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 609\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    604\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mcreate_data_loaders(\n\u001b[1;32m    605\u001b[0m     sequences, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    606\u001b[0m )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 609\u001b[0m train_losses, val_losses, val_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# print(\"\\n=== Evaluating Model ===\")\u001b[39;00m\n\u001b[1;32m    619\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader, k_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 362\u001b[0m, in \u001b[0;36mKeyPredictor.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    359\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m--> 362\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    364\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    365\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, target_windows_size):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.target_windows_size = target_windows_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx] if idx < len(self.sequences) else []\n",
    "        next_windows = self.sequences[idx + 1] if idx + 1 < len(self.sequences) else []\n",
    "\n",
    "        input_windows = windows \n",
    "        target_windows = next_windows[-self.target_windows_size:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        # print(f'training_key_counter{len(target_key_counter)}')\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "\n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        # print (f'torch.tensor(seq_indices, dtype=torch.long).shape{torch.tensor(seq_indices, dtype=torch.long).shape}')\n",
    "        # print (f'torch.tensor(target_padded, dtype=torch.long.shape{torch.tensor(target_padded, dtype=torch.long).shape}')\n",
    "\n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                # print(f'target_set: {target_set}\\n')\n",
    "                # print(f'pred_set: {pred_set}')    \n",
    "                \n",
    "        \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "                # print(f'len(intersection{tp}, target_set_len{len(target_set)}, pred_set{len(pred_set)})')\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        # window_step = max(1, self.window_size // 10)不应该重复呀\n",
    "        window_step = max(1, self.window_size)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length + 1):\n",
    "            seq = windows[i:i + self.sequence_length]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "   \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 计算sigmoid概率\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 基础焦点损失\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        # 'data_file': 'temporal_ycsb_workload_part1.csv',\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 500,             \n",
    "        'sequence_length': 300,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        # 'top_k_hot_keys': 2000,\n",
    "        'batch_size': 16,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 300\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "#device\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事实证明这个是可行的最终方案，大概两个epoch收敛，300万数据预热，要5min，思考一下如何加速吧接下来就。我们现在模拟数据库真实的操作逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Setting up communication with C++...\n",
      "Processing new data from index 0 to 3000000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ming/桌面/PLIN-N /PLIN-N/build/key_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 782\u001b[0m\n\u001b[1;32m    779\u001b[0m         predictor\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 782\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 770\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up communication with C++...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# predictor.setup_communication(\"127.0.0.1\",60001)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# # Start listening for messages in a separate thread\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# comm_thread = threading.Thread(target=predictor.listen_for_messages)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# comm_thread.daemon = True\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# comm_thread.start() 3000000\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;66;03m# Keep the main thread alive\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython predictor is running. Press Ctrl+C to stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 608\u001b[0m, in \u001b[0;36mHotKeyPredictor.train_and_predict\u001b[0;34m(self, start_idx, end_idx)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing new data from index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Read new data from log file\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_idx):\n\u001b[1;32m    610\u001b[0m         line \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ming/桌面/PLIN-N /PLIN-N/build/key_log.csv'"
     ]
    }
   ],
   "source": [
    "# hotkey_predictor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import select\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, target_windows_size):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.target_windows_size = target_windows_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx] if idx < len(self.sequences) else []\n",
    "        next_windows = self.sequences[idx + 1] if idx + 1 < len(self.sequences) else []\n",
    "\n",
    "        input_windows = windows \n",
    "        # target_windows = next_windows\n",
    "        target_windows = next_windows[-self.target_windows_size:]  # bug\n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64, num_layers=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[5000,10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class HotKeyPredictor:\n",
    "    \"\"\"Main class for hot key prediction with communication capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=500, sequence_length=300, top_k_hot_keys=10000, prediction_win=300):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "        \n",
    "        # Communication setup\n",
    "        self.sock = None\n",
    "        self.conn = None\n",
    "        self.addr = None\n",
    "        self.running = False\n",
    "        \n",
    "        # Log file path\n",
    "        self.log_file = \"/home/ming/桌面/PLIN-N /PLIN-N/build/key_log.csv\"\n",
    "        # self.log_file = '/home/ming/桌面/PLIN-N /PLIN-N/code_demo/processed_key.csv'\n",
    "        \n",
    "    def setup_communication(self, host='127.0.0.1', port=60001):\n",
    "        \"\"\"Set up socket communication with C++\"\"\"\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        self.sock.bind((host, port))\n",
    "        self.sock.listen(1)\n",
    "        print(f\"Listening for C++ connection on {host}:{port}\")\n",
    "        \n",
    "        self.conn, self.addr = self.sock.accept()\n",
    "        print(f\"Connected to C++ at {self.addr}\")\n",
    "        \n",
    "    def listen_for_messages(self):\n",
    "        \"\"\"Listen for messages from C++\"\"\"\n",
    "        self.running = True\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Check if data is available\n",
    "                ready = select.select([self.conn], [], [], 0.1)\n",
    "                if ready[0]:\n",
    "                    data = self.conn.recv(1024).decode('utf-8')\n",
    "                    \n",
    "                    if not data:\n",
    "                        continue\n",
    "                        \n",
    "                    if data.startswith(\"INDEX:\"):\n",
    "                        # Parse index range\n",
    "                        parts = data.split(\":\")\n",
    "                        if len(parts) >= 3:\n",
    "                            start_idx = int(parts[1])\n",
    "                            end_idx = int(parts[2])\n",
    "                            \n",
    "                            # Process new data\n",
    "                            self.train_and_predict(start_idx, end_idx)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error in communication: {e}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "        print(\"Starting data preprocessing...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        window_step = max(1, self.window_size)\n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length + 1):\n",
    "            seq = windows[i:i + self.sequence_length]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=16, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"Adaptive heat-enhanced loss\"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # Create multi-hot encoding and count actual hot keys\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # Calculate adaptive emphasis factor\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2\n",
    "        )\n",
    "        \n",
    "        # Calculate sigmoid probability\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Base focal loss\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # Heat enhancement term\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=1, lr=0.001, patience=3):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim=16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate=0.3\n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "      \n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=patience//2, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'key_to_idx': self.key_to_idx,\n",
    "                    'idx_to_key': self.idx_to_key,\n",
    "                    'window_stats': self.window_stats\n",
    "                }, 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists('best_model.pth'):\n",
    "            checkpoint = torch.load('best_model.pth', map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.key_to_idx = checkpoint['key_to_idx']\n",
    "            self.idx_to_key = checkpoint['idx_to_key']\n",
    "            self.window_stats = checkpoint.get('window_stats', {})\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # k_list = [5000,10000]\n",
    "        k_list = [10000]\n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None, top_percentage: float = 0.2) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"Predict hot keys for the next window and return top percentage\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1)\n",
    "            \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        prediction_scores = np.array(prediction_scores)\n",
    "        num_top_keys = int(len(predicted_keys) * top_percentage)\n",
    "        top_predicted_keys = predicted_keys[:num_top_keys]\n",
    "        top_prediction_scores = prediction_scores[:num_top_keys]\n",
    "        \n",
    "        return top_predicted_keys, top_prediction_scores\n",
    "    \n",
    "    # def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "    #     \"\"\"Predict hot keys for the next window\"\"\"\n",
    "    #     if top_k is None:\n",
    "    #         top_k = self.top_k_hot_keys\n",
    "        \n",
    "    #     if len(recent_windows) < self.sequence_length:\n",
    "    #         raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "    #     input_windows = recent_windows[-self.sequence_length:]\n",
    "    #     input_indices = []\n",
    "        \n",
    "    #     for window_keys in input_windows:\n",
    "    #         indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "    #         if len(indices) < self.window_size:\n",
    "    #             indices = indices + [0] * (self.window_size - len(indices))\n",
    "    #         else:\n",
    "    #             indices = indices[:self.window_size]\n",
    "    #         input_indices.append(indices)\n",
    "    #     input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         predictions = self.model(input_tensor)\n",
    "    #         predictions = torch.softmax(predictions, dim=-1)\n",
    "    #         scores, indices = predictions.topk(top_k, dim=-1) #这里也可以是测了top-k的一半\n",
    "            \n",
    "    #         predicted_key = []\n",
    "    #         prediction_scores = []\n",
    "            \n",
    "    #         for i in range(top_k):\n",
    "    #             idx = indices[0, i].item()\n",
    "    #             score = scores[0, i].item()\n",
    "    #             predicted_key.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "    #             prediction_scores.append(score)\n",
    "        \n",
    "    #     return predicted_key, np.array(prediction_scores)\n",
    "    \n",
    "    def get_recent_windows(self):\n",
    "        \"\"\"Get the most recent windows from the log file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.log_file, header=None, names=['key'])\n",
    "            windows = []\n",
    "            total_records = len(df)\n",
    "            window_step = max(1, self.window_size)\n",
    "            start_idx = max(0, total_records - self.window_size * self.sequence_length)\n",
    "            for i in range(start_idx, total_records - self.window_size + 1, window_step):\n",
    "                window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "                windows.append(window_keys)\n",
    "                \n",
    "            # Return the last sequence_length windows\n",
    "            return windows[-self.sequence_length:] if len(windows) >= self.sequence_length else windows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting recent windows: {e}\")\n",
    "            return []\n",
    "        \n",
    "                \n",
    "    def train_and_predict(self, start_idx, end_idx):\n",
    "        \"\"\"Process new data from the log file\"\"\"\n",
    "        print(f\"Processing new data from index {start_idx} to {end_idx}\")\n",
    "            \n",
    "        # Read new data from log file\n",
    "        with open(self.log_file, 'r') as f:\n",
    "            for _ in range(start_idx):\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "            # Read the required lines\n",
    "            keys = []\n",
    "            for i in range(end_idx - start_idx):\n",
    "                line = f.readline().strip()\n",
    "                if line:\n",
    "                    keys.append(line)\n",
    "            \n",
    "        if not keys:\n",
    "            print(\"No new keys to process\")\n",
    "            return\n",
    "                \n",
    "        df = pd.DataFrame(keys, columns=['key'])\n",
    "        sequences = self.preprocess_data(df)\n",
    "            \n",
    "        if self.model is None:\n",
    "            print(\"Training initial model...\")\n",
    "            train_loader, val_loader, test_loader, = self.create_data_loaders(sequences,batch_size=16)\n",
    "            self.train_model(train_loader, val_loader, epochs=2)\n",
    "            self.evaluate_model(test_loader)\n",
    "            self.save_model('initial_model.pth')\n",
    "        # else:\n",
    "        #     print(\"Updating model with new data...\")\n",
    "        #     self.update_model(sequences)\n",
    "                \n",
    "        # Predict hot keys\n",
    "        recent_windows = self.get_recent_windows()\n",
    "        print(f\"Recent windows available for prediction: {len(recent_windows)}\")\n",
    "        \n",
    "        if recent_windows and len(recent_windows) >= self.sequence_length:\n",
    "            hot_keys, scores = self.predict_next_window_hot_keys(recent_windows)\n",
    "            hot_keys_str = \",\".join(hot_keys[::])  # Send hot keys to C++\n",
    "            message = f\"HOT_KEYS:{hot_keys_str}\"\n",
    "            try:\n",
    "                self.conn.send(message.encode('utf-8'))\n",
    "                # print(f\"Sent hot keys: {hot_keys_str}\")\n",
    "            except Exception as e:\n",
    "                    print(f\"Error sending hot keys: {e}\")\n",
    "       \n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    # def update_model(self, sequences):\n",
    "    #     \"\"\"Update the model with new sequences\"\"\"\n",
    "    #     if len(sequences) == 0:\n",
    "    #         return\n",
    "            \n",
    "    #     # Create a small dataset from the new sequences\n",
    "    #     dataset = KeySequenceDataset(sequences, self.key_to_idx, \n",
    "    #                                self.window_size, self.top_k_hot_keys,\n",
    "    #                                self.prediction_win)\n",
    "        \n",
    "    #     # Create a data loader\n",
    "    #     loader = DataLoader(dataset, batch_size=min(8, len(sequences)), \n",
    "    #                       shuffle=True, num_workers=0)\n",
    "        \n",
    "    #     # Fine-tune the model\n",
    "    #     self.fine_tune_model(loader)\n",
    "    \n",
    "    # def fine_tune_model(self, loader, epochs=1, lr=0.0001):\n",
    "    #     \"\"\"Fine-tune the model with new data\"\"\"\n",
    "    #     if self.model is None:\n",
    "    #         return\n",
    "            \n",
    "    #     optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    #     self.model.train()\n",
    "    #     for epoch in range(epochs):\n",
    "    #         for batch_x, batch_y in loader:\n",
    "    #             batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "    #             optimizer.zero_grad()\n",
    "    #             outputs = self.model(batch_x)\n",
    "                \n",
    "    #             loss = self.set_based_loss(outputs, batch_y)\n",
    "    #             loss.backward()\n",
    "                \n",
    "    #             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "    #             optimizer.step()\n",
    "                \n",
    "    #         print(f\"Fine-tuning epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"Load a trained model and metadata\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "        self.key_to_idx = checkpoint['key_to_idx']\n",
    "        self.idx_to_key = checkpoint['idx_to_key']\n",
    "        self.window_size = checkpoint['window_size']\n",
    "        self.sequence_length = checkpoint['sequence_length']\n",
    "        self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "        self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "        vocab_size = checkpoint['vocab_size']\n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim=16,\n",
    "            hidden_size=16,\n",
    "            num_layers=1,\n",
    "            dropout_rate=0.3\n",
    "        ).to(self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the communication thread\"\"\"\n",
    "        self.running = False\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "        if self.sock:\n",
    "            self.sock.close()\n",
    "\n",
    "def main():\n",
    "    # Create predictor instance\n",
    "    predictor = HotKeyPredictor(\n",
    "        # window_size=500,\n",
    "        # sequence_length=300,\n",
    "        # top_k_hot_keys=10000,\n",
    "        # # top_k_hot_keys=4000,\n",
    "        # prediction_win=300\n",
    "        window_size=500,\n",
    "        sequence_length=300,\n",
    "        top_k_hot_keys=10000,\n",
    "        prediction_win=300\n",
    "    )\n",
    "    \n",
    "    # # Check if we have a pre-trained model\n",
    "    # if os.path.exists('best_model.pth'):\n",
    "    #     print(\"Loading pre-trained model...\")\n",
    "    #     predictor.load_model('best_model.pth')\n",
    "    \n",
    "    # Set up communication with C++\n",
    "    print(\"Setting up communication with C++...\")\n",
    "    # predictor.setup_communication(\"127.0.0.1\",60001)\n",
    "    \n",
    "    # # Start listening for messages in a separate thread\n",
    "    # comm_thread = threading.Thread(target=predictor.listen_for_messages)\n",
    "    # comm_thread.daemon = True\n",
    "    # comm_thread.start() 3000000\n",
    "    predictor.train_and_predict(0, 3000000)\n",
    "    \n",
    "    try:\n",
    "        # Keep the main thread alive\n",
    "        print(\"Python predictor is running. Press Ctrl+C to stop.\")\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping predictor...\")\n",
    "        predictor.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "gpu_id = 0                     \n",
    "torch.cuda.empty_cache() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
