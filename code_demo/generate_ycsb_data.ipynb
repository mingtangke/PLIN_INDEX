{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5a5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yiming Guo 07/14/2024\n",
    "\"\"\"\n",
    "./bin/ycsb run basic -p recordcount=100 -p operationcount=500000 -p workload=site.ycsb.workloads.CoreWorkload -p requestdistribution=hotspot -p hotspotdatafraction=0.2 -p hotspotopnfraction=0.8 -p readproportion=1.0 -p insertorder=ordered -p updateproportion=0 -s > data/tracea_load.txt\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635ffd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 是否可用: True\n",
      "当前 CUDA 设备数量: 1\n",
      "当前 CUDA 设备名称: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"CUDA 是否可用:\", torch.cuda.is_available())\n",
    "# print(\"当前 CUDA 设备数量:\", torch.cuda.device_count())\n",
    "# print(\"当前 CUDA 设备名称:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"无\")\n",
    "import torch\n",
    "\n",
    "# 检查 CUDA 可用性\n",
    "print(\"CUDA 是否可用:\", torch.cuda.is_available())\n",
    "print(\"当前 CUDA 设备数量:\", torch.cuda.device_count())\n",
    "\n",
    "# 如果 CUDA 不可用，尝试找出原因\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\nCUDA 不可用的可能原因:\")\n",
    "    \n",
    "    # 检查是否安装了 GPU 版本的 PyTorch\n",
    "    print(\"PyTorch 版本:\", torch.__version__)\n",
    "    \n",
    "    # 检查 CUDA 是否在系统上可用\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode != 0:\n",
    "            print(\"1. 系统中没有 NVIDIA GPU 或 NVIDIA 驱动程序未正确安装\")\n",
    "        else:\n",
    "            print(\"1. 系统中有 NVIDIA GPU，但 PyTorch 可能未编译为 GPU 版本\")\n",
    "    except:\n",
    "        print(\"1. 无法执行 nvidia-smi 命令\")\n",
    "    \n",
    "    # 检查是否安装了正确版本的 PyTorch\n",
    "    print(\"2. 您可能安装了 CPU 版本的 PyTorch，而不是 GPU 版本\")\n",
    "    \n",
    "    # 建议的解决方案\n",
    "    print(\"\\n解决方案:\")\n",
    "    print(\"a. 确保系统中有 NVIDIA GPU\")\n",
    "    print(\"b. 安装 NVIDIA 驱动程序 (使用 nvidia-smi 检查)\")\n",
    "    print(\"c. 安装 CUDA 工具包\")\n",
    "    print(\"d. 安装与您 CUDA 版本匹配的 GPU 版本 PyTorch:\")\n",
    "    print(\"   访问 https://pytorch.org/get-started/locally/ 获取正确的安装命令\")\n",
    "    \n",
    "    # 示例安装命令\n",
    "    print(\"\\n例如，对于 CUDA 11.7:\")\n",
    "    print(\"pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\")\n",
    "else:\n",
    "    print(\"当前 CUDA 设备名称:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6181a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m SEQUENCE_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 输入序列长度(时间窗口数)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m KEYS_PER_WINDOW \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 每个时间窗口跟踪的热键数量\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# device = torch.device(\"cpu\")  # 强制使用CPU\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m使用设备: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "YCSB_PATH = \"/home/ming/桌面/Lion/YCSB\"  # YCSB安装路径\n",
    "OUTPUT_FILE = \"/home/ming/桌面/Lion/ycsb_hotspot_result.txt\"  # 原始结果输出文件\n",
    "PROCESSED_FILE = \"/home/ming/桌面/Lion/lstm_dataset.csv\"  # 处理后的数据集文件\n",
    "LOG_FILE = \"/home/ming/桌面/Lion/query_log.txt\"  # 详细查询日志文件\n",
    "MODEL_PATH = \"/home/ming/桌面/Lion/hotspot_predictor.pth\"  # 模型保存路径\n",
    "RECORD_COUNT = 100  # 数据集大小\n",
    "OPERATION_COUNT = 500000  # 查询操作数量\n",
    "HOTSPOT_FRACTION = 0.2  # 20%的热点键\n",
    "HOTSPOT_OPS_FRACTION = 0.8  # 80%的操作访问热点\n",
    "WINDOW_SIZE_MS = 100  # 时间窗口大小(毫秒)\n",
    "SEQUENCE_LENGTH = 10  # 输入序列长度(时间窗口数)\n",
    "KEYS_PER_WINDOW = 20  # 每个时间窗口跟踪的热键数量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec71d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_ycsb_command():\n",
    "    ycsb_dir = \"/home/ming/桌面/PLIN-N /PLIN-N/YCSB\"\n",
    "    original_dir = os.getcwd()\n",
    "    os.chdir(ycsb_dir)\n",
    "\n",
    "    # # load\n",
    "    # load_command = [\n",
    "    #     \"./bin/ycsb\", \"load\", \"basic\",\n",
    "    #     \"-p\", \"recordcount=10000000\",    # total 10000000 10000000\n",
    "    #     \"-p\", \"operationcount=50000000\",\n",
    "    #     \"-p\", \"workload=site.ycsb.workloads.CoreWorkload\",\n",
    "    #     \"-p\", \"requestdistribution=zipfian\",\n",
    "    #     # \"-p\", \"hotspotdatafraction=0.2\",\n",
    "    #     # \"-p\", \"hotspotopnfraction=0.8\",\n",
    "    #     \"-p\", \"readproportion=1.0\",\n",
    "    #     \"-p\", \"insertorder=ordered\",\n",
    "    #     \"-p\", \"updateproportion=0\",\n",
    "    #     \"-p\",\"readallfields=true\",\n",
    "    #     \"-s\"\n",
    "    # ]\n",
    "    \n",
    "    # with open(\"/home/ming/桌面/PLIN-N /PLIN-N/build/tracea_load.txt\", \"w\") as f:\n",
    "    #     subprocess.run(load_command, stdout=f, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # # run\n",
    "    # run_command = [\n",
    "    #     \"./bin/ycsb\", \"run\", \"basic\",\n",
    "    #     \"-p\", \"recordcount=10000000\",  \n",
    "    #     \"-p\", \"operationcount=50000000\", \n",
    "    #     \"-p\", \"workload=site.ycsb.workloads.CoreWorkload\",\n",
    "    #     \"-p\", \"requestdistribution=zipfian\",\n",
    "    #     # \"-p\", \"hotspotdatafraction=0.2\",\n",
    "    #     # \"-p\", \"hotspotopnfraction=0.8\",\n",
    "    #     \"-p\", \"readproportion=1.0\",\n",
    "    #     \"-p\", \"insertorder=ordered\",\n",
    "    #     \"-p\", \"updateproportion=0\",\n",
    "    #     \"-p\",\"readallfields=true\",\n",
    "    #     \"-s\"\n",
    "    # ]\n",
    "    # f.close()\n",
    "    # load\n",
    "    load_command = [\n",
    "        \"./bin/ycsb\", \"load\", \"basic\",\n",
    "        \"-p\", \"recordcount=1000000\",    # total 10000000 10000000\n",
    "        \"-p\", \"operationcount=25000000\",\n",
    "        \"-p\", \"workload=site.ycsb.workloads.CoreWorkload\",\n",
    "        \"-p\", \"requestdistribution=zipfian\",\n",
    "        # \"-p\", \"hotspotdatafraction=0.2\",\n",
    "        # \"-p\", \"hotspotopnfraction=0.8\",\n",
    "        \"-p\", \"readproportion=1.0\",\n",
    "        \"-p\", \"insertorder=ordered\",\n",
    "        \"-p\", \"updateproportion=0\",\n",
    "        \"-p\",\"readallfields=true\",\n",
    "        \"-s\"\n",
    "    ]\n",
    "    \n",
    "    with open(\"/home/ming/桌面/PLIN-N /PLIN-N/build/tracea_load.txt\", \"w\") as f:\n",
    "        subprocess.run(load_command, stdout=f, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # run\n",
    "    run_command = [\n",
    "        \"./bin/ycsb\", \"run\", \"basic\",\n",
    "        \"-p\", \"recordcount=1000000\",  \n",
    "        \"-p\", \"operationcount=25000000\", \n",
    "        \"-p\", \"workload=site.ycsb.workloads.CoreWorkload\",\n",
    "        \"-p\", \"requestdistribution=zipfian\",\n",
    "        # \"-p\", \"hotspotdatafraction=0.2\",\n",
    "        # \"-p\", \"hotspotopnfraction=0.8\",\n",
    "        \"-p\", \"readproportion=1.0\",\n",
    "        \"-p\", \"insertorder=ordered\",\n",
    "        \"-p\", \"updateproportion=0\",\n",
    "        \"-p\",\"readallfields=true\",\n",
    "        \"-s\"\n",
    "    ]\n",
    "    f.close()\n",
    "    \n",
    "    with open(\"/home/ming/桌面/PLIN-N /PLIN-N/build/tracea_run.txt\", \"w\") as f1:\n",
    "        subprocess.run(run_command, stdout=f1, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "run_ycsb_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract 50000000 keys to /home/ming/桌面/PLIN-N /PLIN-N/build/command.txt.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "input_file = \"/home/ming/桌面/PLIN-N /PLIN-N/build/tracea_run.txt\"\n",
    "output_file = \"/home/ming/桌面/PLIN-N /PLIN-N/build/command_plus.txt\"\n",
    "\n",
    "def extract_keys_from_ycsb_log(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            log_content = f.readlines()\n",
    "        keys = []\n",
    "        for line in log_content:\n",
    "            if 'READ usertable' in line:\n",
    "                match = re.search(r'READ usertable (\\w+)', line)\n",
    "                if match:\n",
    "                    keys.append(\"find \" + match.group(1)[4:])\n",
    "            elif 'UPDATE usertable' in line:\n",
    "                match = re.search(r'UPDATE usertable (\\w+)', line)\n",
    "                if match:\n",
    "                    keys.append(\"find \" + match.group(1)[4:])\n",
    "            elif 'DELETE usertable' in line:\n",
    "                match = re.search(r'DELETE usertable (\\w+)', line)\n",
    "                if match:\n",
    "                    keys.append(\"find \" + match.group(1)[4:])\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for key in keys:\n",
    "                f.write(f\"{key}\\n\")\n",
    "        \n",
    "        print(f\"extract {len(keys)} keys to {output_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"error: cannot find {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"something wrong when process the file: {e}\")\n",
    "\n",
    "extract_keys_from_ycsb_log(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file = \"/home/ming/桌面/PLIN-N /PLIN-N/build/tracea_run.txt\"\n",
    "output_file = \"/home/ming/桌面/PLIN-N /PLIN-N/build/command_plus.txt\"\n",
    "output_file_summary = \"/home/ming/桌面/PLIN-N /PLIN-N/build/processed_key_summary.txt\"\n",
    "\n",
    "def extract_keys_from_ycsb_log(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            log_content = f.readlines()\n",
    "        \n",
    "        key_counts = defaultdict(int)\n",
    "        total_keys = 0\n",
    "        keys = []\n",
    "         \n",
    "        for line in log_content:\n",
    "            match = None\n",
    "            if 'READ usertable' in line:\n",
    "                match = re.search(r'READ usertable (\\w+)', line)\n",
    "            elif 'UPDATE usertable' in line:\n",
    "                match = re.search(r'UPDATE usertable (\\w+)', line)\n",
    "            elif 'DELETE usertable' in line:\n",
    "                match = re.search(r'DELETE usertable (\\w+)', line)\n",
    "            \n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                keys.append(\"find \" + match.group(1)[4:])\n",
    "                if key.startswith(\"user\"):\n",
    "                    key = key[4:]\n",
    "                key_counts[key] += 1\n",
    "                total_keys += 1\n",
    "        \n",
    "\n",
    "        sorted_keys = sorted(key_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"key\\n\")\n",
    "            for key in keys:\n",
    "                f.write(f\"{key}\\n\")\n",
    "        \n",
    "        print(f\"extract {len(keys)} keys to {output_file}\")\n",
    "        \n",
    "        with open(output_file_summary, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"总键访问次数: {total_keys}\\n\")\n",
    "            f.write(f\"唯一键数量: {len(key_counts)}\\n\\n\")\n",
    "            f.write(\"键访问统计 (按访问次数降序):\\n\")\n",
    "            for key, count in sorted_keys:\n",
    "                f.write(f\"{key}: {count}\\n\")\n",
    "        \n",
    "        print(f\"成功处理 {total_keys} 次键访问，涉及 {len(key_counts)} 个唯一键\")\n",
    "        print(f\"结果已保存到: {output_file}\")\n",
    "        # print(\"\\n前10个最常访问的键:\")\n",
    "        # for i, (key, count) in enumerate(sorted_keys[:10], 1):\n",
    "        #     print(f\"{i}. {key}: {count} 次\")\n",
    "       \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 找不到文件 {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出错: {e}\")\n",
    "\n",
    "extract_keys_from_ycsb_log(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query workload...\n",
      "All Keys:100\n",
      "\n",
      "Top 10 Hot Keys:\n",
      "4: 20302 queries (4.06%)\n",
      "10: 20213 queries (4.04%)\n",
      "17: 20177 queries (4.04%)\n",
      "18: 20168 queries (4.03%)\n",
      "13: 20113 queries (4.02%)\n",
      "19: 20102 queries (4.02%)\n",
      "5: 20050 queries (4.01%)\n",
      "1: 20046 queries (4.01%)\n",
      "14: 20041 queries (4.01%)\n",
      "11: 20014 queries (4.00%)\n",
      "0: 19964 queries (3.99%)\n",
      "16: 19944 queries (3.99%)\n",
      "9: 19932 queries (3.99%)\n",
      "12: 19931 queries (3.99%)\n",
      "15: 19900 queries (3.98%)\n",
      "7: 19896 queries (3.98%)\n",
      "8: 19874 queries (3.97%)\n",
      "3: 19870 queries (3.97%)\n",
      "6: 19852 queries (3.97%)\n",
      "2: 19790 queries (3.96%)\n",
      "94: 1327 queries (0.27%)\n",
      "22: 1325 queries (0.27%)\n",
      "27: 1323 queries (0.26%)\n",
      "75: 1321 queries (0.26%)\n",
      "69: 1306 queries (0.26%)\n",
      "99: 1298 queries (0.26%)\n",
      "46: 1298 queries (0.26%)\n",
      "52: 1293 queries (0.26%)\n",
      "45: 1288 queries (0.26%)\n",
      "74: 1287 queries (0.26%)\n",
      "89: 1286 queries (0.26%)\n",
      "21: 1284 queries (0.26%)\n",
      "25: 1282 queries (0.26%)\n",
      "60: 1279 queries (0.26%)\n",
      "31: 1278 queries (0.26%)\n",
      "32: 1276 queries (0.26%)\n",
      "54: 1273 queries (0.25%)\n",
      "90: 1273 queries (0.25%)\n",
      "51: 1272 queries (0.25%)\n",
      "20: 1270 queries (0.25%)\n",
      "80: 1269 queries (0.25%)\n",
      "76: 1267 queries (0.25%)\n",
      "50: 1266 queries (0.25%)\n",
      "70: 1265 queries (0.25%)\n",
      "44: 1264 queries (0.25%)\n",
      "92: 1263 queries (0.25%)\n",
      "96: 1262 queries (0.25%)\n",
      "72: 1261 queries (0.25%)\n",
      "26: 1261 queries (0.25%)\n",
      "97: 1259 queries (0.25%)\n",
      "42: 1258 queries (0.25%)\n",
      "68: 1257 queries (0.25%)\n",
      "87: 1257 queries (0.25%)\n",
      "93: 1256 queries (0.25%)\n",
      "81: 1255 queries (0.25%)\n",
      "36: 1254 queries (0.25%)\n",
      "79: 1251 queries (0.25%)\n",
      "58: 1247 queries (0.25%)\n",
      "38: 1245 queries (0.25%)\n",
      "34: 1245 queries (0.25%)\n",
      "57: 1245 queries (0.25%)\n",
      "84: 1244 queries (0.25%)\n",
      "35: 1243 queries (0.25%)\n",
      "71: 1242 queries (0.25%)\n",
      "29: 1242 queries (0.25%)\n",
      "59: 1240 queries (0.25%)\n",
      "88: 1238 queries (0.25%)\n",
      "85: 1235 queries (0.25%)\n",
      "86: 1235 queries (0.25%)\n",
      "78: 1233 queries (0.25%)\n",
      "33: 1230 queries (0.25%)\n",
      "91: 1228 queries (0.25%)\n",
      "48: 1227 queries (0.25%)\n",
      "23: 1227 queries (0.25%)\n",
      "53: 1225 queries (0.24%)\n",
      "41: 1225 queries (0.24%)\n",
      "65: 1224 queries (0.24%)\n",
      "56: 1224 queries (0.24%)\n",
      "39: 1224 queries (0.24%)\n",
      "83: 1224 queries (0.24%)\n",
      "66: 1223 queries (0.24%)\n",
      "47: 1223 queries (0.24%)\n",
      "63: 1222 queries (0.24%)\n",
      "82: 1221 queries (0.24%)\n",
      "49: 1218 queries (0.24%)\n",
      "67: 1216 queries (0.24%)\n",
      "61: 1214 queries (0.24%)\n",
      "95: 1212 queries (0.24%)\n",
      "77: 1212 queries (0.24%)\n",
      "40: 1210 queries (0.24%)\n",
      "73: 1210 queries (0.24%)\n",
      "37: 1206 queries (0.24%)\n",
      "55: 1204 queries (0.24%)\n",
      "64: 1204 queries (0.24%)\n",
      "30: 1203 queries (0.24%)\n",
      "24: 1202 queries (0.24%)\n",
      "43: 1199 queries (0.24%)\n",
      "98: 1193 queries (0.24%)\n",
      "62: 1188 queries (0.24%)\n",
      "28: 1185 queries (0.24%)\n",
      "\n",
      "Query workload completed in 28.48 seconds\n",
      "Raw output saved to /home/ming/桌面/Lion/ycsb_hotspot_result.txt\n",
      "Query log saved to /home/ming/桌面/Lion/query_log.txt\n",
      "为LSTM处理数据...\n",
      "处理后的数据保存至: /home/ming/桌面/Lion/lstm_dataset.csv\n",
      "总序列数: 193\n",
      "\n",
      "===== 开始训练PyTorch LSTM热点预测模型 =====\n",
      "加载数据集: 193 条序列\n",
      "输入序列形状: torch.Size([193, 10, 20])\n",
      "训练集大小: 154, 批次: 3\n",
      "测试集大小: 39, 批次: 1\n",
      "模型结构:\n",
      "HotspotPredictor(\n",
      "  (embedding): Embedding(20, 64)\n",
      "  (lstm): LSTM(1280, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=400, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ming/anaconda3/envs/dl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 384\u001b[0m\n\u001b[1;32m    381\u001b[0m process_for_lstm()\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# 训练和预测热点键\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m model, key_to_int, int_to_key \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_predict_hotkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m所有处理完成! PyTorch LSTM模型已训练并可用于热点键预测。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 239\u001b[0m, in \u001b[0;36mtrain_and_predict_hotkeys\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m    238\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m--> 239\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[1;32m    242\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# def process_for_lstm():\n",
    "#     \"\"\"处理日志为LSTM训练所需的格式\"\"\"\n",
    "#     print(\"为LSTM处理数据...\")\n",
    "    \n",
    "#     df = pd.read_csv(\n",
    "#         LOG_FILE,\n",
    "#         names=['timestamp', 'key'],\n",
    "#         header=0 \n",
    "#     )\n",
    "    \n",
    "#     df['time_window'] = (df['timestamp'] // WINDOW_SIZE_MS) * WINDOW_SIZE_MS\n",
    "    \n",
    "#     # create time windows and count keys\n",
    "#     window_groups = df.groupby('time_window')['key'].apply(\n",
    "#         lambda x: Counter(x).most_common(KEYS_PER_WINDOW)  # 每个窗口取Top热键\n",
    "#     )\n",
    "    \n",
    "#     lstm_data = []\n",
    "#     window_keys = {} #由hash <window(time), (hot_key,times)>\n",
    "    \n",
    "#     group = 0\n",
    "#     for window, keys in window_groups.items():\n",
    "#         # 确保每个窗口有KEYS_PER_WINDOW个键\n",
    "#         if not group:\n",
    "#             group += 1\n",
    "#             continue\n",
    "#         key_list = [key for key, _ in keys] #只关注提取热键本身，不关注其热键的次数\n",
    "#         if len(key_list) < KEYS_PER_WINDOW:\n",
    "#             key_list += ['0'] * (KEYS_PER_WINDOW - len(key_list))\n",
    "#         window_keys[window] = key_list[:KEYS_PER_WINDOW]  # 确保长度一致\n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "#     windows = sorted(window_keys.keys()) #a list sorted by time, is the windows name\n",
    "#     windows_keys = {} as hash <window,hot_keys>\n",
    "#     \"\"\"\n",
    "#     windows = sorted(window_keys.keys()) \n",
    "#     # for i in range(len(windows)):\n",
    "#     #     print(f\"窗口 {i}: {windows[i]} - 热键: {window_keys[windows[i]]}\")\n",
    "    \n",
    "#     for i in range(len(windows) - SEQUENCE_LENGTH):\n",
    "#         input_seq = []\n",
    "#         for j in range(SEQUENCE_LENGTH):\n",
    "#             win = windows[i + j]\n",
    "#             input_seq.append(window_keys[win])\n",
    "#         # print(f\"round {i + SEQUENCE_LENGTH}\")\n",
    "#         target_win = windows[i + SEQUENCE_LENGTH]\n",
    "#         target = window_keys[target_win]\n",
    "        \n",
    "#         lstm_data.append({\n",
    "#             \"input_sequence\": input_seq,\n",
    "#             \"target\": target,\n",
    "#             \"start_window\": windows[i],\n",
    "#             \"end_window\": windows[i + SEQUENCE_LENGTH - 1],\n",
    "#             \"target_window\": target_win\n",
    "#         })\n",
    "    \n",
    "#     # 保存处理后的数据\n",
    "#     with open(PROCESSED_FILE, 'w') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\"sequence\", \"target\", \"start_window\", \"end_window\", \"target_window\"])\n",
    "        \n",
    "#         for item in lstm_data:\n",
    "#             # 将序列转换为字符串表示，确保所有元素都是字符串类型\n",
    "#             seq_str = \";\".join([\",\".join(str(key) for key in keys) for keys in item[\"input_sequence\"]])\n",
    "#             target_str = \",\".join(str(key) for key in item[\"target\"])\n",
    "            \n",
    "#             writer.writerow([\n",
    "#                 seq_str,\n",
    "#                 target_str,\n",
    "#                 item[\"start_window\"],\n",
    "#                 item[\"end_window\"],\n",
    "#                 item[\"target_window\"]\n",
    "#             ])\n",
    "    \n",
    "#     print(f\"处理后的数据保存至: {PROCESSED_FILE}\")\n",
    "#     print(f\"总序列数: {len(lstm_data)}\")\n",
    "    \n",
    "#     return lstm_data\n",
    "\n",
    "# # process_for_lstm()\n",
    "# class HotspotPredictor(nn.Module):\n",
    "#     \"\"\"PyTorch LSTM模型用于热点键预测\"\"\"\n",
    "#     def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=embedding_dim * KEYS_PER_WINDOW,\n",
    "#             hidden_size=hidden_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True  # 添加双向LSTM\n",
    "#         )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(2*hidden_dim if num_layers > 1 else hidden_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, KEYS_PER_WINDOW * vocab_size)  # 预测多个键\n",
    "#         )\n",
    "#         # self.fc = nn.Sequential(\n",
    "#         #     nn.Linear(2*hidden_dim, 256),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.Dropout(0.3),\n",
    "#         #     nn.Linear(256, vocab_size)  # 只预测单个热键\n",
    "#         # )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, keys = x.size()\n",
    "#         x = self.embedding(x)  # (batch, seq, keys, emb)\n",
    "#         x = x.view(batch_size, seq_len, -1)  # (batch, seq, keys*emb)\n",
    "        \n",
    "#         lstm_out, _ = self.lstm(x)  # (batch, seq, hidden)\n",
    "#         lstm_out = lstm_out[:, -1, :]  # 取最后时间步\n",
    "\n",
    "#         output = self.fc(lstm_out)\n",
    "#         return output  # [batch, vocab_size]\n",
    "        \n",
    "#         # output = self.fc(lstm_out)\n",
    "#         # return output.view(batch_size, KEYS_PER_WINDOW, -1)  # (batch, KEYS, vocab)\n",
    "\n",
    "# # ===== PyTorch 数据加载器 =====\n",
    "# class HotspotDataset(Dataset):\n",
    "#     \"\"\"PyTorch数据集加载器\"\"\"\n",
    "#     def __init__(self, sequences, targets):\n",
    "#         self.sequences = sequences\n",
    "#         self.targets = targets\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.sequences[idx], self.targets[idx]\n",
    "# # ===== PyTorch 训练和预测 =====\n",
    "# def train_and_predict_hotkeys():\n",
    "#     \"\"\"使用PyTorch LSTM模型训练和预测热点键\"\"\"\n",
    "#     plt.rcParams['axes.unicode_minus'] = False \n",
    "#     print(\"\\n===== 开始训练PyTorch LSTM热点预测模型 =====\")\n",
    "    \n",
    "#     # 1. 加载处理后的数据\n",
    "#     if not os.path.exists(PROCESSED_FILE):\n",
    "#         print(f\"错误：处理后的数据文件 {PROCESSED_FILE} 不存在\")\n",
    "#         return\n",
    "    \n",
    "#     data = pd.read_csv(PROCESSED_FILE)\n",
    "#     print(f\"加载数据集: {len(data)} 条序列\")\n",
    "    \n",
    "#     # 2. 数据预处理\n",
    "#     all_keys = set()\n",
    "#     for seq in data['sequence']:\n",
    "#         windows = seq.split(';')\n",
    "#         for window in windows:\n",
    "#             keys = window.split(',')\n",
    "#             all_keys.update(keys)\n",
    "#     all_keys.add('0')\n",
    "    \n",
    "#     key_to_int = {key: i for i, key in enumerate(sorted(all_keys))}\n",
    "#     int_to_key = {i: key for key, i in key_to_int.items()}\n",
    "#     vocab_size = len(all_keys)\n",
    "    \n",
    "#     # 3. 准备训练数据\n",
    "#     sequences = []\n",
    "#     targets = []\n",
    "    \n",
    "#     for i, row in data.iterrows():\n",
    "#         seq_str = row['sequence']\n",
    "#         target_str = row['target']\n",
    "#         seq_tensor = []\n",
    "#         windows = seq_str.split(';')\n",
    "#         for window in windows:\n",
    "#             keys = window.split(',')\n",
    "#             window_ints = [key_to_int.get(key, key_to_int['0']) for key in keys[:KEYS_PER_WINDOW]]\n",
    "#             seq_tensor.append(window_ints)\n",
    "        \n",
    "#         # # only take the first key as target\n",
    "#         # target_key = target_str.split(',')[0]\n",
    "#         # target_int = key_to_int.get(target_key, key_to_int['0'])\n",
    "#         target_keys = target_str.split(',')[:KEYS_PER_WINDOW]\n",
    "#         target_ints = [key_to_int.get(k, key_to_int['0']) for k in target_keys]\n",
    "#         targets.append(target_ints)\n",
    "        \n",
    "#         sequences.append(seq_tensor)\n",
    "#         # targets.append(target_int)\n",
    "    \n",
    "#     # transform to PyTorch tensors\n",
    "#     sequences_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "#     targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    \n",
    "#     print(f\"输入序列形状: {sequences_tensor.shape}\")\n",
    "    \n",
    "#     # 4. classify the data into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         sequences_tensor, targets_tensor, test_size=0.2, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # create DataLoader\n",
    "#     train_dataset = HotspotDataset(X_train, y_train)\n",
    "#     test_dataset = HotspotDataset(X_test, y_test)\n",
    "    \n",
    "#     batch_size = 64\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     print(f\"训练集大小: {len(train_dataset)}, 批次: {len(train_loader)}\")\n",
    "#     print(f\"测试集大小: {len(test_dataset)}, 批次: {len(test_loader)}\")\n",
    "    \n",
    "#     # 5. initialize the model\n",
    "#     model = HotspotPredictor(\n",
    "#         vocab_size=vocab_size,\n",
    "#         embedding_dim=64,\n",
    "#         hidden_dim=128,\n",
    "#         num_layers=2\n",
    "#     ).to(device)\n",
    "    \n",
    "#     print(\"模型结构:\")\n",
    "#     print(model)\n",
    "    \n",
    "#     # loss function and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "#     # 6. begin training the model\n",
    "#     print(\"\\n开始训练模型...\")\n",
    "#     num_epochs = 40\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "#     val_accuracies = []\n",
    "#     top5_accuracies = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "#             # 前向传播\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # 反向传播和优化\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "#         # 计算平均训练损失\n",
    "#         epoch_loss = running_loss / len(train_dataset)\n",
    "#         train_losses.append(epoch_loss)\n",
    "        \n",
    "#         # 验证阶段\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         top5_correct = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in test_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 # loss = criterion(outputs, labels)\n",
    "#                 loss = 0\n",
    "#                 for i in range(KEYS_PER_WINDOW):\n",
    "#                     loss += criterion(outputs[:, i* vocab_size : (i+1)*vocab_size], labels[:, i])\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "                \n",
    "#                 _, top5_preds = torch.topk(outputs, 15, dim=1)\n",
    "#                 top5_correct += sum([1 for i in range(labels.size(0)) if labels[i] in top5_preds[i]])\n",
    "        \n",
    "#         val_loss = val_loss / len(test_dataset)\n",
    "#         val_losses.append(val_loss)\n",
    "#         accuracy = correct / total\n",
    "#         top5_accuracy = top5_correct / total\n",
    "#         top5_accuracies.append(top5_accuracy)\n",
    "#         val_accuracies.append(accuracy)\n",
    "        \n",
    "#         # 更新学习率\n",
    "#         scheduler.step(val_loss)\n",
    "        \n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "#               f\"训练损失: {epoch_loss:.4f} | 验证损失: {val_loss:.4f} | \"\n",
    "#               f\"准确率: {accuracy:.4f} | Top-5准确率: {top5_accuracy:.4f}\")\n",
    "    \n",
    "#     # 绘制训练曲线\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.plot(train_losses, label='train_loss')\n",
    "#     plt.plot(val_losses, label='validation_loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.legend()\n",
    "#     plt.title('training and validation loss')\n",
    "    \n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.plot(val_accuracies, label='validation_accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.title('validation accuracy')\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.plot(top5_accuracies, label='Top-5 accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.legend() \n",
    "#     plt.title('Top-5 accuracy over epochs')    \n",
    "    \n",
    "#     # plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     # plt.savefig(\"/home/ming/Lion/training_metrics.png\")\n",
    "#     # plt.close()\n",
    "    \n",
    "#     # 7. 保存模型\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'key_to_int': key_to_int,\n",
    "#         'int_to_key': int_to_key,\n",
    "#         'vocab_size': vocab_size\n",
    "#     }, MODEL_PATH)\n",
    "#     print(f\"模型已保存至: {MODEL_PATH}\")\n",
    "\n",
    "#     # 8. 使用模型进行预测\n",
    "#     def predict_next_hotkeys(model, sequence, key_to_int, top_k=20):\n",
    "#         \"\"\"预测下一个窗口最可能的热键\"\"\"\n",
    "#         # 准备输入数据\n",
    "#         seq_tensor = []\n",
    "#         windows = sequence.split(';')\n",
    "#         for window in windows:\n",
    "#             keys = window.split(',')\n",
    "#             window_ints = [key_to_int.get(key, key_to_int['0']) for key in keys[:KEYS_PER_WINDOW]]\n",
    "#             seq_tensor.append(window_ints)\n",
    "        \n",
    "#         input_tensor = torch.tensor([seq_tensor], dtype=torch.long).to(device)\n",
    "        \n",
    "#         # 进行预测\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             output = model(input_tensor)\n",
    "#             probabilities = torch.softmax(output[0], dim=0) #类别预测的概率\n",
    "        \n",
    "#         # 获取top_k预测\n",
    "#         top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "#         top_keys = [(int_to_key[i.item()], top_probs[j].item()) \n",
    "#                    for j, i in enumerate(top_indices)]\n",
    "        \n",
    "#         return top_keys\n",
    "    \n",
    "#     # 示例预测\n",
    "#     if len(data) > 0:\n",
    "#         sample_sequence = data.iloc[0]['sequence']\n",
    "#         predicted = predict_next_hotkeys(model, sample_sequence, key_to_int)\n",
    "        \n",
    "#         actual_target = data.iloc[0]['target'].split(',')[0]\n",
    "        \n",
    "#         print(\"\\n示例预测:\")\n",
    "#         print(f\"输入序列: {sample_sequence}\")\n",
    "#         print(f\"实际下一个热键: {actual_target}\")\n",
    "#         print(\"预测的下一个热键 (概率):\")\n",
    "#         for key, prob in predicted:\n",
    "#             print(f\"  {key}: {prob:.4f}\")\n",
    "    \n",
    "#     return model, key_to_int, int_to_key\n",
    "# # ===== 主执行流程 =====\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 确保输出目录存在\n",
    "#     # os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "#     # # 清空旧文件\n",
    "#     for file_path in [OUTPUT_FILE, LOG_FILE, PROCESSED_FILE]:\n",
    "#         if os.path.exists(file_path):\n",
    "#             os.remove(file_path)\n",
    "            \n",
    "#     run_query_workload()\n",
    "#     process_for_lstm()\n",
    "    \n",
    "#     # 训练和预测热点键\n",
    "#     model, key_to_int, int_to_key = train_and_predict_hotkeys()\n",
    "    \n",
    "#     print(\"\\n所有处理完成! PyTorch LSTM模型已训练并可用于热点键预测。\")\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
