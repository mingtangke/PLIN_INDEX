{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 5000\n",
      "  sequence_length: 100\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 8\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 50\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 10000000 operations in 0.53 seconds\n",
      "Using device: cpu\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 10000000\n",
      "Window size: 5000 keys, Step: 1250 keys\n",
      "Created 7997 windows with overlap\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, prediction_win):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.prediction_win = prediction_win\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx]\n",
    "\n",
    "        input_windows = windows[:-self.prediction_win]  \n",
    "        target_windows = windows[-self.prediction_win:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        # print(f'training_key_counter{len(target_key_counter)}')\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "\n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        # print (f'torch.tensor(seq_indices, dtype=torch.long).shape{torch.tensor(seq_indices, dtype=torch.long).shape}')\n",
    "        # print (f'torch.tensor(target_padded, dtype=torch.long.shape{torch.tensor(target_padded, dtype=torch.long).shape}')\n",
    "\n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                # print(f'target_set: {target_set}\\n')\n",
    "                # print(f'pred_set: {pred_set}')    \n",
    "                \n",
    "        \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "                # print(f'len(intersection{tp}, target_set_len{len(target_set)}, pred_set{len(pred_set)})')\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        # window_step = max(1, self.window_size // 10)  # 10% \n",
    "        window_step = max(1, self.window_size // 4)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length - self.prediction_win + 1):\n",
    "            seq = windows[i:i + self.sequence_length + self.prediction_win]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "   \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 计算sigmoid概率\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 基础焦点损失\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        # 'data_file': 'temporal_ycsb_workload_part1.csv',\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 5000,             \n",
    "        'sequence_length': 100,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        # 'top_k_hot_keys': 2000,\n",
    "        'batch_size': 8,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 50\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "#device\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "     \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "gpu_id = 0                     \n",
    "torch.cuda.empty_cache() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n",
    "### sudo nvidia-smi --gpu-reset -i 0\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 5000\n",
      "  sequence_length: 100\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 8\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 50\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 10000000 operations in 0.51 seconds\n",
      "Using device: cuda\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 10000000\n",
      "Window size: 5000 keys, Step: 1250 keys\n",
      "Created 7997 windows with overlap\n",
      "\n",
      "Data Statistics:\n",
      "  Total unique keys: 100000\n",
      "  Total windows: 7997\n",
      "  Avg unique keys per window: 3842.83\n",
      "  Max unique keys per window: 3956\n",
      "  Min unique keys per window: 3735\n",
      "\n",
      "Creating training sequences with 100 historical windows...\n",
      "Created 7848 training sequences in 4.38 seconds\n",
      "\n",
      "Creating data loaders...\n",
      "  Train samples: 5493\n",
      "  Validation samples: 785\n",
      "  Test samples: 1570\n",
      "Data loaders created in 0.00 seconds\n",
      "\n",
      "=== Training Model ===\n",
      "\n",
      "Initializing efficient model with vocabulary size: 100001\n",
      "Model parameters: 15,144,417 total, 15,144,417 trainable\n",
      "\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5] - 343.86s\n",
      "  Train Loss: 0.4104, Val Loss: 0.2871, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.3471  prec: 0.3471  f1: 0.3471  accu: 0.3471  \n",
      "\n",
      "Epoch [2/5] - 376.03s\n",
      "  Train Loss: 0.2786, Val Loss: 0.2865, LR: 0.001000\n",
      "  Validation Metrics:\n",
      "  Top-10000: reca: 0.3477  prec: 0.3477  f1: 0.3477  accu: 0.3477  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 706\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# model_path = 'hot_key_prsedictor.pth'\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# predictor.save_model(model_path)\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     \n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# print(f\"\\nTraining completed! Model saved to {model_path}\")\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 687\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mcreate_data_loaders(\n\u001b[1;32m    683\u001b[0m     sequences, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    684\u001b[0m )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 687\u001b[0m train_losses, val_losses, val_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# print(\"\\n=== Evaluating Model ===\")\u001b[39;00m\n\u001b[1;32m    697\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader, k_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 451\u001b[0m, in \u001b[0;36mKeyPredictor.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    448\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    449\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 451\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     train_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    454\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m train_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, prediction_win):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.prediction_win = prediction_win\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx]\n",
    "\n",
    "        input_windows = windows[:-self.prediction_win]  \n",
    "        target_windows = windows[-self.prediction_win:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        # print(f'training_key_counter{len(target_key_counter)}')\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "\n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        # print (f'torch.tensor(seq_indices, dtype=torch.long).shape{torch.tensor(seq_indices, dtype=torch.long).shape}')\n",
    "        # print (f'torch.tensor(target_padded, dtype=torch.long.shape{torch.tensor(target_padded, dtype=torch.long).shape}')\n",
    "\n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64,num_layers = 2,dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout= dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, window_size]\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    " \n",
    "\n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                # print(f'target_set: {target_set}\\n')\n",
    "                # print(f'pred_set: {pred_set}')    \n",
    "                \n",
    "        \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "                # print(f'len(intersection{tp}, target_set_len{len(target_set)}, pred_set{len(pred_set)})')\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        # window_step = max(1, self.window_size // 10)  # 10% \n",
    "        window_step = max(1, self.window_size // 4)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "\n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length - self.prediction_win + 1):\n",
    "            seq = windows[i:i + self.sequence_length + self.prediction_win]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def set_based_loss(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Custom loss for set-based prediction\n",
    "        predictions: (batch_size, vocab_size)\n",
    "        targets: (batch_size, top_k) - indices of hot keys\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # Create multi-hot target vector\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, device=predictions.device)\n",
    "        \n",
    "        # Fill in target indices (ignore padding)\n",
    "        mask = targets != 0\n",
    "        if mask.any():\n",
    "            row_indices = torch.arange(batch_size, device=targets.device).unsqueeze(1).expand_as(targets)[mask]\n",
    "            col_indices = targets[mask]\n",
    "            target_multihot[row_indices, col_indices] = 1\n",
    "        # print (f\"predictions{predictions}\")\n",
    "        # print (f\"target_multihot{target_multihot}\")\n",
    "        # Use multi-label soft margin loss\n",
    "        loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "        loss = loss_fn(predictions, target_multihot)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    # def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "    #     \"\"\"\n",
    "    #     自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "    #     Args:\n",
    "    #         predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "    #         targets: 目标热键索引 (batch_size, top_k)\n",
    "    #         base_emphasis: 基础强调因子\n",
    "    #         max_emphasis: 最大强调因子\n",
    "        \n",
    "    #     Returns:\n",
    "    #         自适应热度增强损失值\n",
    "    #     \"\"\"\n",
    "    #     batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "    #     # 创建目标的多热编码并统计实际热键数量\n",
    "    #     target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "    #                                 device=predictions.device, \n",
    "    #                                 dtype=torch.float)\n",
    "    #     actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "    #     for i in range(batch_size):\n",
    "    #         actual_keys = targets[i][targets[i] != 0]\n",
    "    #         if len(actual_keys) > 0:\n",
    "    #             target_multihot[i, actual_keys] = 1.0\n",
    "    #             actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "    #     # 计算自适应强调因子\n",
    "    #     # 热键数量越少，强调因子越大\n",
    "    #     avg_key_count = actual_key_counts.float().mean()\n",
    "    #     emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "    #         - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "    #     )\n",
    "        \n",
    "    #     # 计算sigmoid概率\n",
    "    #     pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "    #     # 基础焦点损失\n",
    "    #     alpha = 0.25\n",
    "    #     gamma = 2.0\n",
    "    #     bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "    #     p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "    #     modulating_factor = (1 - p_t) ** gamma\n",
    "    #     alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "    #     focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "    #     # 热度增强项\n",
    "    #     # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "    #     heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "    #     # 组合损失\n",
    "    #     total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "    #     return total_loss.mean()\n",
    "\n",
    "    # def set_based_loss(self, predictions, targets, rank_weight=2.0):\n",
    "    #     \"\"\"\n",
    "    #     排名感知焦点损失，确保真实热键在预测排名中位于前列\n",
    "        \n",
    "    #     Args:\n",
    "    #         predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "    #         targets: 目标热键索引 (batch_size, top_k)\n",
    "    #         rank_weight: 排名权重因子，控制排名重要性的程度\n",
    "        \n",
    "    #     Returns:\n",
    "    #         排名感知焦点损失值\n",
    "    #     \"\"\"\n",
    "    #     batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "    #     # 创建目标的多热编码\n",
    "    #     target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "    #                                 device=predictions.device, \n",
    "    #                                 dtype=torch.float)\n",
    "        \n",
    "    #     for i in range(batch_size):\n",
    "    #         actual_keys = targets[i][targets[i] != 0]\n",
    "    #         if len(actual_keys) > 0:\n",
    "    #             target_multihot[i, actual_keys] = 1.0\n",
    "        \n",
    "    #     # 计算sigmoid概率\n",
    "    #     pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "    #     # 基础焦点损失\n",
    "    #     alpha = 0.25\n",
    "    #     gamma = 2.0\n",
    "    #     bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "    #     p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "    #     modulating_factor = (1 - p_t) ** gamma\n",
    "    #     alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "    #     focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "    #     # 排名感知项\n",
    "    #     # 获取每个样本的预测排名\n",
    "    #     _, indices = predictions.sort(dim=1, descending=True)\n",
    "    #     ranks = torch.zeros_like(predictions)\n",
    "    #     for i in range(batch_size):\n",
    "    #         ranks[i, indices[i]] = torch.arange(vocab_size, device=predictions.device).float()\n",
    "        \n",
    "    #     # 计算真实热键的排名损失\n",
    "    #     # 排名越高（值越小），损失越小\n",
    "    #     hot_key_ranks = ranks * target_multihot\n",
    "    #     rank_loss = hot_key_ranks / vocab_size  # 归一化到[0,1]\n",
    "        \n",
    "    #     # 组合焦点损失和排名损失\n",
    "    #     total_loss = focal_loss + rank_weight * rank_loss\n",
    "        \n",
    "    #     return total_loss.mean()\n",
    " \n",
    "  \n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim = 16,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            dropout_rate = 0.3 \n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []  # Store all metrics for each epoch\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                loss = self.set_based_loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    loss = self.set_based_loss(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "            \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[20000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(input_tensor)  # (1, vocab_size)\n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "#     def load_model(self, filepath: str):\n",
    "#         \"\"\"Load a trained model and metadata\"\"\"\n",
    "#         checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "#         self.key_to_idx = checkpoint['key_to_idx']\n",
    "#         self.idx_to_key = checkpoint['idx_to_key']\n",
    "#         self.window_size = checkpoint['window_size']\n",
    "#         self.sequence_length = checkpoint['sequence_length']\n",
    "#         self.top_k_hot_keys = checkpoint['top_k_hot_keys']\n",
    "#         self.window_stats = checkpoint.get('window_stats', {})\n",
    "        \n",
    "#         vocab_size = checkpoint['vocab_size']\n",
    "#         self.model = EfficientKeyPredictionLSTM(\n",
    "#             vocab_size=vocab_size,\n",
    "#             window_size=self.window_size,\n",
    "#             num_layers=2\n",
    "#         ).to(self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "#         print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        # 'data_file': 'temporal_ycsb_workload_part1.csv',\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 5000,             \n",
    "        'sequence_length': 100,         \n",
    "        'top_k_hot_keys': 10000,   # in train 100000 ,the hottest is 20000  \n",
    "        # 'top_k_hot_keys': 2000,\n",
    "        'batch_size': 8,      \n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 50\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience']\n",
    "    )\n",
    "    \n",
    "    # visualize_training(train_losses, val_losses, val_metrics_history, 'training_progress.png')\n",
    "    \n",
    "    # print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    # model_path = 'hot_key_prsedictor.pth'\n",
    "    # predictor.save_model(model_path)\n",
    "    \n",
    "    # print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "     \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient LSTM Hot Key Predictor ===\n",
      "Configuration:\n",
      "  data_file: processed_key.csv\n",
      "  window_size: 5000\n",
      "  sequence_length: 100\n",
      "  top_k_hot_keys: 10000\n",
      "  batch_size: 16\n",
      "  epochs: 5\n",
      "  learning_rate: 0.001\n",
      "  patience: 10\n",
      "  prediction_win: 50\n",
      "  grad_accum_steps: 2\n",
      "\n",
      "Loading workload data from processed_key.csv...\n",
      "Loaded 10000000 operations in 0.60 seconds\n",
      "Using device: cuda\n",
      "Starting data preprocessing with fixed-size windows...\n",
      "Total records: 10000000\n",
      "Window size: 5000 keys, Step: 1250 keys\n",
      "Created 7997 windows with overlap\n",
      "\n",
      "Data Statistics:\n",
      "  Total unique keys: 100000\n",
      "  Total windows: 7997\n",
      "  Avg unique keys per window: 3842.83\n",
      "  Max unique keys per window: 3956\n",
      "  Min unique keys per window: 3735\n",
      "\n",
      "Creating training sequences with 100 historical windows...\n",
      "Created 7848 training sequences in 5.04 seconds\n",
      "\n",
      "Creating data loaders...\n",
      "  Train samples: 5493\n",
      "  Validation samples: 785\n",
      "  Test samples: 1570\n",
      "Data loaders created in 0.00 seconds\n",
      "\n",
      "=== Training Model ===\n",
      "\n",
      "Initializing efficient model with vocabulary size: 100001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 676\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed! Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 676\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 659\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    654\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mcreate_data_loaders(\n\u001b[1;32m    655\u001b[0m     sequences, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 659\u001b[0m train_losses, val_losses, val_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrad_accum_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Evaluating Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader, k_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 331\u001b[0m, in \u001b[0;36mKeyPredictor.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, patience, grad_accum_steps)\u001b[0m\n\u001b[1;32m    328\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# 使用更小的模型\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientKeyPredictionLSTM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 减少嵌入维度\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 减少隐藏层大小\u001b[39;49;00m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# 减少LSTM层数\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 增加dropout防止过拟合\u001b[39;49;00m\n\u001b[1;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    341\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from torch.cuda.amp import autocast, GradScaler  # 混合精度训练\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KeySequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for key sequence prediction with full sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, key_to_idx, window_size, top_k_hot_keys, prediction_win):\n",
    "        self.sequences = sequences\n",
    "        self.key_to_idx = key_to_idx\n",
    "        self.window_size = window_size\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.vocab_size = len(key_to_idx)\n",
    "        self.prediction_win = prediction_win\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        windows = self.sequences[idx]\n",
    "\n",
    "        input_windows = windows[:-self.prediction_win]  \n",
    "        target_windows = windows[-self.prediction_win:]  \n",
    "        \n",
    "        seq_indices = []\n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            seq_indices.append(indices)\n",
    "        \n",
    "        target_keys_flattened = []\n",
    "        for window in target_windows:\n",
    "            target_keys_flattened.extend(window)\n",
    "        \n",
    "        target_key_counter = Counter(target_keys_flattened)\n",
    "        target_hot_keys = [key for key, _ in target_key_counter.most_common(self.top_k_hot_keys)]\n",
    "        \n",
    "        target_indices = [self.key_to_idx.get(key, 0) for key in target_hot_keys]\n",
    "        target_padded = target_indices + [0] * max(0, self.top_k_hot_keys - len(target_indices))\n",
    "        target_padded = target_padded[:self.top_k_hot_keys]\n",
    "        \n",
    "        return torch.tensor(seq_indices, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)\n",
    "\n",
    "class EfficientKeyPredictionLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=32, hidden_size=64, num_layers=2, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * window_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  \n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, win_size = x.shape\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = x.view(batch * seq_len, win_size)\n",
    "        emb = self.embedding(x)  # [batch*seq_len, win_size, emb_dim]\n",
    "        emb = emb.view(batch, seq_len, -1)  # [batch, seq_len, win_size*emb_dim]\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        last_out = self.dropout(last_out)\n",
    "\n",
    "        return self.fc(last_out)\n",
    " \n",
    "class KeyAccuracyMetrics:\n",
    "    \n",
    "    def __init__(self, k_list=[10000]):\n",
    "        self.k_list = k_list\n",
    "        \n",
    "    def __call__(self, predictions, targets):\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        results = {}\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            _, pred_indices = predictions.topk(k, dim=-1)\n",
    "            \n",
    "            total_recall = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                non_zero_targets = targets[i][targets[i] != 0]\n",
    "                if len(non_zero_targets) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                target_set = set(non_zero_targets.tolist())\n",
    "                pred_set = set(pred_indices[i].tolist())\n",
    "                \n",
    "                intersection = pred_set & target_set\n",
    "                tp = len(intersection)\n",
    "            \n",
    "                recall = tp / len(target_set) if len(target_set) > 0 else 0.0\n",
    "                precision = tp / k if k > 0 else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                accuracy = tp / min(len(target_set), k) if min(len(target_set), k) > 0 else 0.0\n",
    "                \n",
    "                total_recall += recall\n",
    "                total_precision += precision\n",
    "                total_f1 += f1\n",
    "                total_accuracy += accuracy\n",
    "                total_samples += 1\n",
    "            \n",
    "            if total_samples > 0:\n",
    "                results[f'top_{k}_recall'] = total_recall / total_samples\n",
    "                results[f'top_{k}_precision'] = total_precision / total_samples\n",
    "                results[f'top_{k}_f1'] = total_f1 / total_samples\n",
    "                results[f'top_{k}_accuracy'] = total_accuracy / total_samples\n",
    "            else:\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    results[f'top_{k}_{metric}'] = 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeyPredictor:\n",
    "    \"\"\"Main class for key prediction using LSTM with complete sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20, sequence_length=10, top_k_hot_keys=5, prediction_win=2):\n",
    "        self.window_size = window_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.top_k_hot_keys = top_k_hot_keys\n",
    "        self.key_to_idx = {}\n",
    "        self.idx_to_key = {}\n",
    "        self.model = None\n",
    "        self.prediction_win = prediction_win\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # For data statistics\n",
    "        self.window_stats = {}\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> List:\n",
    "        print(\"Starting data preprocessing with fixed-size windows...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        windows = []\n",
    "        total_records = len(df)\n",
    "        window_step = max(1, self.window_size // 4)  \n",
    "        \n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Window size: {self.window_size} keys, Step: {window_step} keys\")\n",
    "        \n",
    "        for i in range(0, total_records - self.window_size + 1, window_step):\n",
    "            window_keys = df['key'].iloc[i:i+self.window_size].tolist()\n",
    "            windows.append(window_keys)\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows with overlap\")\n",
    "        \n",
    "        # Create key vocabulary from all keys\n",
    "        all_keys = set()\n",
    "        for key_list in windows:\n",
    "            all_keys.update(key_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        sorted_keys = sorted(all_keys)\n",
    "        self.key_to_idx = {key: idx+1 for idx, key in enumerate(sorted_keys)}  # 0 reserved for padding\n",
    "        self.key_to_idx[\"[PAD]\"] = 0\n",
    "        self.idx_to_key = {idx: key for key, idx in self.key_to_idx.items()}\n",
    "        \n",
    "        keys_per_window = [len(set(key_list)) for key_list in windows]  # unique keys per window\n",
    "        self.window_stats = {\n",
    "            'total_unique_keys': len(all_keys),\n",
    "            'total_windows': len(windows),\n",
    "            'avg_unique_keys_per_window': np.mean(keys_per_window),\n",
    "            'max_unique_keys_per_window': np.max(keys_per_window),\n",
    "            'min_unique_keys_per_window': np.min(keys_per_window),\n",
    "            'window_size': self.window_size,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nData Statistics:\")\n",
    "        print(f\"  Total unique keys: {self.window_stats['total_unique_keys']}\")\n",
    "        print(f\"  Total windows: {self.window_stats['total_windows']}\")\n",
    "        print(f\"  Avg unique keys per window: {self.window_stats['avg_unique_keys_per_window']:.2f}\")\n",
    "        print(f\"  Max unique keys per window: {self.window_stats['max_unique_keys_per_window']}\")\n",
    "        print(f\"  Min unique keys per window: {self.window_stats['min_unique_keys_per_window']}\")\n",
    "        \n",
    "        sequences = []\n",
    "        \n",
    "        print(f\"\\nCreating training sequences with {self.sequence_length} historical windows...\")\n",
    "     \n",
    "        for i in range(len(windows) - self.sequence_length - self.prediction_win + 1):\n",
    "            seq = windows[i:i + self.sequence_length + self.prediction_win]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        print(f\"Created {len(sequences)} training sequences in {time.time()-start_time:.2f} seconds\")\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def create_data_loaders(self, sequences, batch_size=64, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Create train/val/test data loaders\"\"\"\n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        start_time = time.time()\n",
    "        total_samples = len(sequences)\n",
    "        train_end = int(total_samples * (1 - test_size - val_size))\n",
    "        val_end = int(total_samples * (1 - test_size))\n",
    "        \n",
    "        train_seq = sequences[:train_end]\n",
    "        val_seq = sequences[train_end:val_end]\n",
    "        test_seq = sequences[val_end:]\n",
    "        \n",
    "        print(f\"  Train samples: {len(train_seq)}\")\n",
    "        print(f\"  Validation samples: {len(val_seq)}\")\n",
    "        print(f\"  Test samples: {len(test_seq)}\")\n",
    "        \n",
    "        # 使用更高效的DataLoader配置\n",
    "        train_dataset = KeySequenceDataset(train_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        val_dataset = KeySequenceDataset(val_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    "        test_dataset = KeySequenceDataset(test_seq, self.key_to_idx, self.window_size, self.top_k_hot_keys, self.prediction_win)\n",
    " \n",
    "        # 增加num_workers并使用pin_memory加速数据加载\n",
    "        num_workers = min(4, os.cpu_count() // 2)  # 使用一半的CPU核心\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "        \n",
    "        print(f\"Data loaders created in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def set_based_loss(self, predictions, targets, base_emphasis=1.0, max_emphasis=3.0):\n",
    "        \"\"\"\n",
    "        自适应热度增强损失，根据实际热键数量动态调整热度增强程度\n",
    "        \n",
    "        Args:\n",
    "            predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "            targets: 目标热键索引 (batch_size, top_k)\n",
    "            base_emphasis: 基础强调因子\n",
    "            max_emphasis: 最大强调因子\n",
    "        \n",
    "        Returns:\n",
    "            自适应热度增强损失值\n",
    "        \"\"\"\n",
    "        batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "        # 创建目标的多热编码并统计实际热键数量\n",
    "        target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "                                    device=predictions.device, \n",
    "                                    dtype=torch.float)\n",
    "        actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            actual_keys = targets[i][targets[i] != 0]\n",
    "            if len(actual_keys) > 0:\n",
    "                target_multihot[i, actual_keys] = 1.0\n",
    "                actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "        # 计算自适应强调因子\n",
    "        # 热键数量越少，强调因子越大\n",
    "        avg_key_count = actual_key_counts.float().mean()\n",
    "        emphasis_factor = base_emphasis + (max_emphasis - base_emphasis) * torch.sigmoid(\n",
    "            - (avg_key_count - 5) / 2  # 当平均热键数量为5时，强调因子为(base_emphasis + max_emphasis)/2\n",
    "        )\n",
    "        \n",
    "        # 使用BCEWithLogitsLoss而不是手动计算Sigmoid+BCE\n",
    "        bce_with_logits_loss = nn.BCEWithLogitsLoss(reduction='none')(predictions, target_multihot)\n",
    "        \n",
    "        # 计算概率用于调制因子和热度增强\n",
    "        pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "        # 焦点损失计算\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "        modulating_factor = (1 - p_t) ** gamma\n",
    "        alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_with_logits_loss\n",
    "        \n",
    "        # 热度增强项\n",
    "        # 对真实热键的预测概率与1之间的差异进行惩罚，程度由强调因子控制\n",
    "        heat_enhancement = target_multihot * (1 - pred_prob) ** emphasis_factor\n",
    "        \n",
    "        # 组合损失\n",
    "        total_loss = focal_loss + heat_enhancement\n",
    "        \n",
    "        return total_loss.mean()\n",
    "  \n",
    "    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15, grad_accum_steps=4):\n",
    "        \"\"\"Train the LSTM model with optimizations\"\"\"\n",
    "        vocab_size = len(self.key_to_idx)\n",
    "        print(f\"\\nInitializing efficient model with vocabulary size: {vocab_size}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 使用更小的模型\n",
    "        self.model = EfficientKeyPredictionLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            window_size=self.window_size,\n",
    "            embedding_dim=16,  # 减少嵌入维度\n",
    "            hidden_size=32,    # 减少隐藏层大小\n",
    "            num_layers=1,      # 减少LSTM层数\n",
    "            dropout_rate=0.3   # 增加dropout防止过拟合\n",
    "        ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        # 使用余弦退火学习率调度器配合预热\n",
    "        warmup_epochs = 2\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=lr,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(train_loader) // grad_accum_steps,\n",
    "            pct_start=warmup_epochs/epochs,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        # 混合精度训练\n",
    "        scaler = GradScaler() if self.device.type == 'cuda' else None\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_metrics_history = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "     \n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=[self.top_k_hot_keys])\n",
    "        \n",
    "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            # 梯度累积\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                # 在训练循环中\n",
    "                if scaler:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(batch_x)\n",
    "                        loss = self.set_based_loss(outputs, batch_y) / grad_accum_steps\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # 每grad_accum_steps步更新一次权重\n",
    "                    if (i + 1) % grad_accum_steps == 0:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                        scheduler.step()\n",
    "                else:\n",
    "                    outputs = self.model(batch_x)\n",
    "                    loss = self.set_based_loss(outputs, batch_y) / grad_accum_steps\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % grad_accum_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        scheduler.step()\n",
    "                # # 混合精度训练\n",
    "                # if scaler:\n",
    "                #     with autocast():\n",
    "                #         outputs = self.model(batch_x)\n",
    "                #         loss = self.set_based_loss(outputs, batch_y) / grad_accum_steps\n",
    "                    \n",
    "                #     scaler.scale(loss).backward()\n",
    "                    \n",
    "                #     # 每grad_accum_steps步更新一次权重\n",
    "                #     if (i + 1) % grad_accum_steps == 0:\n",
    "                #         scaler.unscale_(optimizer)\n",
    "                #         torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                #         scaler.step(optimizer)\n",
    "                #         scaler.update()\n",
    "                #         optimizer.zero_grad()\n",
    "                #         scheduler.step()\n",
    "                # else:\n",
    "                #     outputs = self.model(batch_x)\n",
    "                #     loss = self.set_based_loss(outputs, batch_y) / grad_accum_steps\n",
    "                #     loss.backward()\n",
    "                    \n",
    "                #     if (i + 1) % grad_accum_steps == 0:\n",
    "                #         torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                #         optimizer.step()\n",
    "                #         optimizer.zero_grad()\n",
    "                #         scheduler.step()\n",
    "                \n",
    "                train_loss += loss.item() * grad_accum_steps\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_metrics = defaultdict(float)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    \n",
    "                    if scaler:\n",
    "                        with autocast():\n",
    "                            outputs = self.model(batch_x)\n",
    "                            loss = self.set_based_loss(outputs, batch_y)\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x)\n",
    "                        loss = self.set_based_loss(outputs, batch_y)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    batch_metrics = metrics_calculator(outputs, batch_y)\n",
    "                    for key, value in batch_metrics.items():\n",
    "                        val_metrics[key] += value\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "            \n",
    "            # Average metrics\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] /= val_batches if val_batches > 0 else 1\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics_history.append(dict(val_metrics))\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_key_predictor.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress with metrics\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}] - {time.time()-epoch_start:.2f}s\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "            print(\"  Validation Metrics:\")\n",
    "            for k in [self.top_k_hot_keys]:\n",
    "                print(f\"  Top-{k}: \", end=\"\")\n",
    "                for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                    key = f'top_{k}_{metric}'\n",
    "                    value = val_metrics.get(key, 0)\n",
    "                    print(f\"{metric[:4]}: {value:.4f}  \", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        if os.path.exists('best_key_predictor.pth'):\n",
    "            self.model.load_state_dict(torch.load('best_key_predictor.pth'))\n",
    "            print(\"Loaded best model weights\")\n",
    "        \n",
    "        print(f\"\\nTraining completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return train_losses, val_losses, val_metrics_history\n",
    "    \n",
    "    def evaluate_model(self, test_loader, k_list=[10000]):\n",
    "        \"\"\"Evaluate the model with various top-k metrics\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        metrics_calculator = KeyAccuracyMetrics(k_list=k_list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                # 使用混合精度进行评估\n",
    "                if self.device.type == 'cuda':\n",
    "                    with autocast():\n",
    "                        outputs = self.model(batch_x)\n",
    "                else:\n",
    "                    outputs = self.model(batch_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_dict = metrics_calculator(outputs, batch_y)\n",
    "                for key, value in metrics_dict.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # Average metrics\n",
    "        results = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            results[key] = np.mean(values)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in k_list:\n",
    "            print(f\"Top-{k} Metrics:\")\n",
    "            for metric in ['recall', 'precision', 'f1', 'accuracy']:\n",
    "                key = f'top_{k}_{metric}'\n",
    "                value = results.get(key, 0)\n",
    "                print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Evaluation completed in {time.time()-start_time:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def predict_next_window_hot_keys(self, recent_windows: List[List[str]], top_k: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k_hot_keys\n",
    "        \n",
    "        if len(recent_windows) < self.sequence_length:\n",
    "            raise ValueError(f\"Need at least {self.sequence_length} recent windows, got {len(recent_windows)}\")\n",
    "        \n",
    "        # Take the last sequence_length windows\n",
    "        input_windows = recent_windows[-self.sequence_length:]\n",
    "        input_indices = []\n",
    "        \n",
    "        for window_keys in input_windows:\n",
    "            indices = [self.key_to_idx.get(key, 0) for key in window_keys]\n",
    "            if len(indices) < self.window_size:\n",
    "                indices = indices + [0] * (self.window_size - len(indices))\n",
    "            else:\n",
    "                indices = indices[:self.window_size]\n",
    "            input_indices.append(indices)\n",
    "        \n",
    "        # Create batch\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.device.type == 'cuda':\n",
    "                with autocast():\n",
    "                    predictions = self.model(input_tensor)\n",
    "            else:\n",
    "                predictions = self.model(input_tensor)\n",
    "                \n",
    "            predictions = torch.softmax(predictions, dim=-1)\n",
    "            scores, indices = predictions.topk(top_k, dim=-1) \n",
    "            predicted_keys = []\n",
    "            prediction_scores = []\n",
    "            \n",
    "            for i in range(top_k):\n",
    "                idx = indices[0, i].item()\n",
    "                score = scores[0, i].item()\n",
    "                predicted_keys.append(self.idx_to_key.get(idx, \"[UNK]\"))\n",
    "                prediction_scores.append(score)\n",
    "        \n",
    "        return predicted_keys, np.array(prediction_scores)\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model and metadata\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'key_to_idx': self.key_to_idx,\n",
    "            'idx_to_key': self.idx_to_key,\n",
    "            'window_size': self.window_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'top_k_hot_keys': self.top_k_hot_keys,\n",
    "            'vocab_size': len(self.key_to_idx),\n",
    "            'window_stats': self.window_stats\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    # 配置参数\n",
    "    config = {\n",
    "        'data_file': 'processed_key.csv',    \n",
    "        'window_size': 5000,             \n",
    "        'sequence_length': 100,         \n",
    "        'top_k_hot_keys': 10000,\n",
    "        'batch_size': 16,  # 增加批次大小\n",
    "        'epochs': 5,          \n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 10,         \n",
    "        'prediction_win': 50,\n",
    "        'grad_accum_steps': 2  # 梯度累积步数\n",
    "    }\n",
    "    \n",
    "    print(\"=== Efficient LSTM Hot Key Predictor ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLoading workload data from {config['data_file']}...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(config['data_file'])\n",
    "    print(f\"Loaded {len(df)} operations in {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "    predictor = KeyPredictor(\n",
    "        window_size=config['window_size'],\n",
    "        sequence_length=config['sequence_length'],\n",
    "        top_k_hot_keys=config['top_k_hot_keys'],\n",
    "        prediction_win=config['prediction_win']  \n",
    "    )\n",
    "\n",
    "    sequences = predictor.preprocess_data(df)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Error: No valid sequences created. Check your data and parameters.\")\n",
    "        return\n",
    "    \n",
    "    train_loader, val_loader, test_loader = predictor.create_data_loaders(\n",
    "        sequences, batch_size=config['batch_size']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    train_losses, val_losses, val_metrics_history = predictor.train_model(\n",
    "        train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['learning_rate'],\n",
    "        patience=config['patience'],\n",
    "        grad_accum_steps=config['grad_accum_steps']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Evaluating Model ===\")\n",
    "    metrics = predictor.evaluate_model(test_loader, k_list=[10000])\n",
    "    \n",
    "    model_path = 'hot_key_predictor.pth'\n",
    "    predictor.save_model(model_path)\n",
    "    \n",
    "    print(f\"\\nTraining completed! Model saved to {model_path}\")\n",
    "     \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def set_based_loss(self, predictions, targets):\n",
    "#         \"\"\"\n",
    "#         Custom loss for set-based prediction\n",
    "#         predictions: (batch_size, vocab_size)\n",
    "#         targets: (batch_size, top_k) - indices of hot keys\n",
    "#         \"\"\"\n",
    "#         batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "#         # Create multi-hot target vector\n",
    "#         target_multihot = torch.zeros(batch_size, vocab_size, device=predictions.device)\n",
    "        \n",
    "#         # Fill in target indices (ignore padding)\n",
    "#         mask = targets != 0\n",
    "#         if mask.any():\n",
    "#             row_indices = torch.arange(batch_size, device=targets.device).unsqueeze(1).expand_as(targets)[mask]\n",
    "#             col_indices = targets[mask]\n",
    "#             target_multihot[row_indices, col_indices] = 1\n",
    "#         # print (f\"predictions{predictions}\")\n",
    "#         # print (f\"target_multihot{target_multihot}\")\n",
    "#         # Use multi-label soft margin loss\n",
    "#         loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "#         loss = loss_fn(predictions, target_multihot)\n",
    "        \n",
    "#         return loss\n",
    "    \n",
    "#     def set_based_loss(self, predictions, targets):\n",
    "#         batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "#         # 创建目标的多热编码并统计实际热键数量\n",
    "#         target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "#                                     device=predictions.device, \n",
    "#                                     dtype=torch.float)\n",
    "#         actual_key_counts = torch.zeros(batch_size, device=predictions.device)\n",
    "        \n",
    "#         for i in range(batch_size):\n",
    "#             actual_keys = targets[i][targets[i] != 0]\n",
    "#             if len(actual_keys) > 0:\n",
    "#                 target_multihot[i, actual_keys] = 1.0\n",
    "#                 actual_key_counts[i] = len(actual_keys)\n",
    "        \n",
    "#         # 动态调整焦点损失参数\n",
    "#         # 热键数量越少，越关注正样本（提高alpha，降低gamma）\n",
    "#         avg_key_count = actual_key_counts.mean()\n",
    "#         alpha = 0.5 / (1 + torch.exp(-(avg_key_count - 5)))  # 自适应alpha\n",
    "#         gamma = 2.0 - 0.5 / (1 + torch.exp(-(avg_key_count - 5)))  # 自适应gamma\n",
    "        \n",
    "#         # 计算焦点损失\n",
    "#         pred_prob = torch.sigmoid(predictions)\n",
    "#         bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "        \n",
    "#         p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "#         modulating_factor = (1 - p_t) ** gamma\n",
    "#         alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "        \n",
    "#         focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "#         return focal_loss.mean()\n",
    "\n",
    "    # def set_based_loss(self, predictions, targets, rank_weight=2.0):\n",
    "    #     \"\"\"\n",
    "    #     排名感知焦点损失，确保真实热键在预测排名中位于前列\n",
    "        \n",
    "    #     Args:\n",
    "    #         predictions: 模型预测得分 (batch_size, vocab_size)\n",
    "    #         targets: 目标热键索引 (batch_size, top_k)\n",
    "    #         rank_weight: 排名权重因子，控制排名重要性的程度\n",
    "        \n",
    "    #     Returns:\n",
    "    #         排名感知焦点损失值\n",
    "    #     \"\"\"\n",
    "    #     batch_size, vocab_size = predictions.shape\n",
    "        \n",
    "    #     # 创建目标的多热编码\n",
    "    #     target_multihot = torch.zeros(batch_size, vocab_size, \n",
    "    #                                 device=predictions.device, \n",
    "    #                                 dtype=torch.float)\n",
    "        \n",
    "    #     for i in range(batch_size):\n",
    "    #         actual_keys = targets[i][targets[i] != 0]\n",
    "    #         if len(actual_keys) > 0:\n",
    "    #             target_multihot[i, actual_keys] = 1.0\n",
    "        \n",
    "    #     # 计算sigmoid概率\n",
    "    #     pred_prob = torch.sigmoid(predictions)\n",
    "        \n",
    "    #     # 基础焦点损失\n",
    "    #     alpha = 0.25\n",
    "    #     gamma = 2.0\n",
    "    #     bce_loss = nn.BCELoss(reduction='none')(pred_prob, target_multihot)\n",
    "    #     p_t = target_multihot * pred_prob + (1 - target_multihot) * (1 - pred_prob)\n",
    "    #     modulating_factor = (1 - p_t) ** gamma\n",
    "    #     alpha_factor = target_multihot * alpha + (1 - target_multihot) * (1 - alpha)\n",
    "    #     focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "        \n",
    "    #     # 排名感知项\n",
    "    #     # 获取每个样本的预测排名\n",
    "    #     _, indices = predictions.sort(dim=1, descending=True)\n",
    "    #     ranks = torch.zeros_like(predictions)\n",
    "    #     for i in range(batch_size):\n",
    "    #         ranks[i, indices[i]] = torch.arange(vocab_size, device=predictions.device).float()\n",
    "        \n",
    "    #     # 计算真实热键的排名损失\n",
    "    #     # 排名越高（值越小），损失越小\n",
    "    #     hot_key_ranks = ranks * target_multihot\n",
    "    #     rank_loss = hot_key_ranks / vocab_size  # 归一化到[0,1]\n",
    "        \n",
    "    #     # 组合焦点损失和排名损失\n",
    "    #     total_loss = focal_loss + rank_weight * rank_loss\n",
    "        \n",
    "    #     return total_loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
